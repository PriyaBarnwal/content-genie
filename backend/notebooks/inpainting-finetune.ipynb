{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6464ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xformers\n",
    "# !pip install torchvision transformers accelerate sentencepiece wandb bitsandbytes omegaconf datasets\n",
    "# !pip install git+https://github.com/huggingface/diffusers\n",
    "# !pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 --index-url https://download.pytorch.org/whl/cu117\n",
    "# 71374ee1894829db638938029d7510f5fcc09ddb # wandb api key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c03c5",
   "metadata": {},
   "source": [
    "## Finetuning StabilityAI/stable-diffusion-2-inpainting model for Arbonne Lotion dataset using LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f76adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionInpaintPipeline, UNet2DConditionModel\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8ca35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"inpainting_dataset/arbonne-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc51bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl_no</th>\n",
       "      <th>image_name</th>\n",
       "      <th>mask_name</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.png</td>\n",
       "      <td>1_mask.jpg</td>\n",
       "      <td>a photo of body lotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.png</td>\n",
       "      <td>2_mask.jpg</td>\n",
       "      <td>a photo of two body lotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.png</td>\n",
       "      <td>3_mask.jpg</td>\n",
       "      <td>a photo of two body lotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.png</td>\n",
       "      <td>4_mask.jpg</td>\n",
       "      <td>a photo of two body lotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.png</td>\n",
       "      <td>5_mask.jpg</td>\n",
       "      <td>a photo of two body lotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sl_no image_name   mask_name                       prompt\n",
       "0      1      1.png  1_mask.jpg       a photo of body lotion\n",
       "1      2      2.png  2_mask.jpg  a photo of two body lotions\n",
       "2      3      3.png  3_mask.jpg  a photo of two body lotions\n",
       "3      4      4.png  4_mask.jpg  a photo of two body lotions\n",
       "4      5      5.png  5_mask.jpg  a photo of two body lotions"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb8095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"image_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27be8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mask_and_masked_image(image, mask):\n",
    "    image = np.array(image)\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "\n",
    "    mask = np.array(mask.convert(\"L\"))\n",
    "    mask = mask.astype(np.float32) / 255.0\n",
    "    mask = mask[None, None]\n",
    "    mask[mask < 0.5] = 0\n",
    "    mask[mask >= 0.5] = 1\n",
    "    mask = torch.from_numpy(mask)\n",
    "\n",
    "    masked_image = image * (mask < 0.5)\n",
    "\n",
    "    return mask, masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8a9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision = \"fp16\" # or \"bf16\"\n",
    "report_to = \"wandb\"\n",
    "output_dir = \"arbonne-model\"\n",
    "batch_size = 1\n",
    "max_train_steps = None\n",
    "checkpointing_steps = 100\n",
    "train_epochs = 50\n",
    "gradient_accumulation_steps = 2\n",
    "lr_warmup_steps = 0\n",
    "learning_rate=5e-06\n",
    "max_grad_norm = 1.0\n",
    "global_step = 0\n",
    "first_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7057205",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    mixed_precision=mixed_precision,\n",
    "    log_with=report_to\n",
    ")\n",
    "    \n",
    "weight_dtype = torch.float32\n",
    "# if accelerator.mixed_precision == \"fp16\":\n",
    "#     weight_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f5472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name, subfolder=\"unet\")\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "unet.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f7dcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_images_path,\n",
    "        masks_path,\n",
    "        metadata,\n",
    "        tokenizer,\n",
    "        tokenizer_max_length,\n",
    "        size=586,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.images = metadata[\"image_name\"]\n",
    "        self.masks = metadata[\"mask_name\"]\n",
    "        self.instance_prompt = metadata[\"prompt\"]\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "        self.instance_images_path = instance_images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = Image.open(self.instance_images_path + \"/\" + self.images[index])\n",
    "        mask_image = Image.open(self.masks_path + \"/\" + self.masks[index]).convert(\"L\")\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "\n",
    "        example[\"PIL_images\"] = instance_image\n",
    "        example[\"mask_images\"] = mask_image\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt[index],\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer_max_length,\n",
    "        ).input_ids\n",
    "        \n",
    "        mask, masked_image = prepare_mask_and_masked_image(example[\"PIL_images\"], example[\"mask_images\"])\n",
    "        \n",
    "        example[\"masks\"] = mask\n",
    "        example[\"masked_images\"] = masked_image\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd4f53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"inpainting_dataset/images-no-text\"\n",
    "masks_dir = \"inpainting_dataset/masks\"\n",
    "\n",
    "train_ds = DreamBoothDataset(instance_images_path=images_dir, masks_path=masks_dir, metadata=df, tokenizer=tokenizer, tokenizer_max_length=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9b3754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIL_images': <PIL.Image.Image image mode=RGB size=586x586>,\n",
       " 'mask_images': <PIL.Image.Image image mode=L size=586x586>,\n",
       " 'instance_images': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'instance_prompt_ids': [49406, 320, 1125, 539, 1774, 25260, 49407],\n",
       " 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'masked_images': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "648dfe43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 586, 586]), torch.Size([1, 1, 586, 586]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][\"instance_images\"].shape, train_ds[0][\"masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fcdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    masks = [example[\"masks\"] for example in examples]\n",
    "    masked_images = [example[\"masked_images\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
    "    masks = torch.stack(masks)\n",
    "    masked_images = torch.stack(masked_images)\n",
    "    batch = {\"input_ids\": input_ids, \"pixel_values\": pixel_values, \"masks\": masks, \"masked_images\": masked_images}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9da109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e40b0fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 9]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 7]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 7]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 9]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 9]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 9]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 9]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 9]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 7]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n",
      "torch.Size([1, 7]) torch.Size([1, 3, 586, 586]) torch.Size([1, 1, 1, 586, 586]) torch.Size([1, 1, 3, 586, 586])\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(train_dataloader):\n",
    "    print(j[\"input_ids\"].shape, j[\"pixel_values\"].shape, j[\"masks\"].shape, j[\"masked_images\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a98061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set correct lora layers\n",
    "\n",
    "lora_attn_procs = {}\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "\n",
    "unet.set_attn_processor(lora_attn_procs)\n",
    "lora_layers = AttnProcsLayers(unet.attn_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "517da777",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_class = bnb.optim.AdamW8bit\n",
    "learning_rate = (learning_rate * gradient_accumulation_steps * batch_size * accelerator.num_processes)\n",
    "\n",
    "# Optimizer creation\n",
    "params_to_optimize = lora_layers.parameters()\n",
    "optimizer = optimizer_class(params_to_optimize, lr=learning_rate)\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309887ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Train Steps : 300\n"
     ]
    }
   ],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "import math\n",
    "\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "print(f\"Max Train Steps : {max_train_steps}\")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,\n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060234de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "total_batch_size = batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "\n",
    "lora_layers, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    lora_layers, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8ec97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreyassk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/Diffusion/wandb/run-20230811_103334-af9b5rdo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shreyassk/arbonne-lora/runs/af9b5rdo' target=\"_blank\">atomic-snow-37</a></strong> to <a href='https://wandb.ai/shreyassk/arbonne-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shreyassk/arbonne-lora' target=\"_blank\">https://wandb.ai/shreyassk/arbonne-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shreyassk/arbonne-lora/runs/af9b5rdo' target=\"_blank\">https://wandb.ai/shreyassk/arbonne-lora/runs/af9b5rdo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if accelerator.is_main_process:\n",
    "    accelerator.init_trackers(\"arbonne-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4def675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 9, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/300 [00:01<?, ?it/s, lr=1e-5, step_loss=0.00132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 9, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/300 [00:02<10:56,  2.20s/it, lr=1e-5, step_loss=0.012]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 9, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/300 [00:02<10:56,  2.20s/it, lr=1e-5, step_loss=0.00145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 7, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   1%|          | 2/300 [00:03<07:33,  1.52s/it, lr=1e-5, step_loss=0.00544]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 7, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   1%|          | 2/300 [00:03<07:33,  1.52s/it, lr=1e-5, step_loss=0.0355] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 7, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   1%|          | 3/300 [00:04<06:27,  1.31s/it, lr=1e-5, step_loss=0.00482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 9, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   1%|          | 3/300 [00:04<06:27,  1.31s/it, lr=1e-5, step_loss=0.00942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 7, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   1%|▏         | 4/300 [00:05<05:56,  1.20s/it, lr=1e-5, step_loss=0.0242] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 73, 73])\n",
      "torch.Size([1, 9, 73, 73])\n",
      "torch.Size([1, 7, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   1%|▏         | 4/300 [00:05<05:56,  1.20s/it, lr=1e-5, step_loss=0.00377]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Add noise to the model input according to the noise magnitude at each timestep\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# (this is the forward diffusion process)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m noisy_latents \u001b[38;5;241m=\u001b[39m \u001b[43mnoise_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(noisy_latents\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Concatentate the noised latents with the mask and masked latents\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/diffusers/schedulers/scheduling_ddpm.py:461\u001b[0m, in \u001b[0;36mDDPMScheduler.add_noise\u001b[0;34m(self, original_samples, noise, timesteps)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_noise\u001b[39m(\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    456\u001b[0m     original_samples: torch\u001b[38;5;241m.\u001b[39mFloatTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Make sure alphas_cumprod and timestep have same device and dtype as original_samples\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     alphas_cumprod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malphas_cumprod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mto(original_samples\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    464\u001b[0m     sqrt_alpha_prod \u001b[38;5;241m=\u001b[39m alphas_cumprod[timesteps] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Steps\")\n",
    "\n",
    "resolution = 586\n",
    "for epoch in range(first_epoch, train_epochs):\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Convert images to latent space\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "            \n",
    "            # Convert masked images to latent space\n",
    "            masked_latents = vae.encode(batch[\"masked_images\"].reshape(batch[\"pixel_values\"].shape).to(dtype=weight_dtype)).latent_dist.sample()\n",
    "            masked_latents = masked_latents * vae.config.scaling_factor\n",
    "            \n",
    "            pixel_values = batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
    "            model_input = pixel_values\n",
    "            \n",
    "            masks = batch[\"masks\"]\n",
    "            # resize the mask to latents shape as we concatenate the mask to the latents\n",
    "            mask = torch.stack([\n",
    "                torch.nn.functional.interpolate(mask, size=(resolution // 8, resolution // 8))\n",
    "                for mask in masks\n",
    "            ])\n",
    "            mask = mask.reshape(-1, 1, resolution // 8, resolution // 8)\n",
    "            \n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            \n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
    "            )\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the model input according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            print(noisy_latents.shape)\n",
    "            \n",
    "            # Concatentate the noised latents with the mask and masked latents\n",
    "            latent_model_input = torch.cat([noisy_latents, mask, masked_latents], dim=1)\n",
    "            \n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "            \n",
    "            # Predict the noise residual\n",
    "            print(latent_model_input.shape)\n",
    "            print(encoder_hidden_states.shape)\n",
    "            noise_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "            \n",
    "            # Get the target for loss depending on the prediction type\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(model_input, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "            loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n",
    "            avg_loss = accelerator.gather(loss.repeat(batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / gradient_accumulation_steps\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                params_to_clip = lora_layers.parameters()\n",
    "                accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            train_loss = 0.0\n",
    "       \n",
    "            if global_step % checkpointing_steps == 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "# Save the lora layers\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    unet = unet.to(torch.float32)\n",
    "    unet.save_attn_procs(output_dir)\n",
    "    \n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04244b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
